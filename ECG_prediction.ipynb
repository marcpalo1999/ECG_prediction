{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO : \n",
    "\n",
    "    - Separate data into metadata df and signal dict, so that it is easier to analyse data and to access signal just with key (sample_id)\n",
    "    -  Check for normal ecgs and train the classifier with AF vs normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V > full_requirements.txt && pip list --format=freeze >> full_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb  # To read the .hea file\n",
    "import scipy.io as sio  # To read .mat files\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, lfilter\n",
    "import numpy as np\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import signal \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Directories\n",
    "dataset_dir = './a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/WFDBRecords/03/032'\n",
    "g_leads = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
    "fs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disease_map.loc[28189009, 'Full Name']\n",
    "def disease_map_func(diagnosis):\n",
    "    disease_map_path = './a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/ConditionNames_SNOMED-CT.csv'\n",
    "    disease_map = pd.read_csv(disease_map_path)\n",
    "    diagnosis = [disease_map.loc[int(code), \"Full Name\"] for code in diagnosis.split(',')]\n",
    "    return diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ecg_data = {}\n",
    "patient_data = {}\n",
    "\n",
    "# Loop through each record in the dataset directory\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    if filename.endswith(\".hea\"):  # Process .hea files\n",
    "        record_name = filename.split(\".\")[0]\n",
    "\n",
    "        # Read the header (.hea) file\n",
    "        record = wfdb.rdheader(os.path.join(dataset_dir, record_name))\n",
    "\n",
    "        # Extract metadata\n",
    "        age = None\n",
    "        sex = None\n",
    "        diagnosis = None\n",
    "        for comment in record.comments:\n",
    "            if comment.startswith(\"Age:\"):\n",
    "                age = comment.split(\":\")[1].strip()\n",
    "            if comment.startswith(\"Sex:\"):\n",
    "                sex = comment.split(\":\")[1].strip()\n",
    "            if comment.startswith(\"Dx:\"):\n",
    "                diagnosis = comment.split(\":\")[1].strip()  # This gives you the SNOMED codes\n",
    "        \n",
    "        # Read the 12-lead ECG signal from the .mat file\n",
    "        mat_file_path = os.path.join(dataset_dir, f\"{record_name}.mat\")\n",
    "        mat_data = sio.loadmat(mat_file_path)\n",
    "        ecg_signals = mat_data['val']  # 'val' typically holds the ECG signal in PhysioNet datasets\n",
    "        ecg_signals= pd.DataFrame(ecg_signals.T, columns  = g_leads)\n",
    "        # Store the data in the dictionary\n",
    "        ecg_data[record_name] = {\n",
    "            \"ecg_signals\": ecg_signals,  # 12-lead ECG signals\n",
    "            }\n",
    "        patient_data[record_name] = {\n",
    "            \"diagnosis_code\": diagnosis,  # Disease label (SNOMED codes)\n",
    "            \"diagnosis_name\": disease_map_func(diagnosis),\n",
    "            \"age\": age,\n",
    "            \"sex\": sex\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import scipy\n",
    "import random\n",
    "from scipy.signal import find_peaks, welch\n",
    "\n",
    "def temp_freq_plot(signal, title):\n",
    "\n",
    "    fig,ax = plt.subplots(12,2, figsize=(20,12*4))\n",
    "    for i in range(0,12):\n",
    "        _key = g_leads[i]\n",
    "        #ax[i].set_title(_key)\n",
    "        ax[i,0].plot(signal[_key], color='black', linewidth=0.6)\n",
    "        ax[i,0].set_ylim((-1500,1500))\n",
    "        ax[i,0].set_xticks(   np.arange(0,5001,500)  )   \n",
    "        ax[i,0].set_xticklabels(   np.arange(0,5001,500)/fs  )   \n",
    "        ax[i,0].grid(axis='x')\n",
    "        ax[i,0].annotate(_key,(-200,0))\n",
    "        #ax[i].set_xlabel('Time(sec)')\n",
    "        #ax[i].set_ylabel('mV')\n",
    "        ax[i,0].hlines(0,0,5000,color='black', linewidth=0.3)\n",
    "\n",
    "        frequencies, psd_values = welch( signal[_key], fs, nperseg=1024)\n",
    "\n",
    "        # Plotting the estimated PSD\n",
    "        ax[i,1].semilogy(frequencies, psd_values)\n",
    "        ax[i,1].set_title('Power Spectral Density (PSD) Estimate using Welch\\'s Method')\n",
    "        ax[i,1].set_xlabel('Frequency (Hz)')\n",
    "        ax[i,1].set_ylabel('PSD (V^2/Hz)')\n",
    "\n",
    "    plt.suptitle(f\"{title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='lowpass', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Design a Notch Filter to remove 60 Hz power line interference\n",
    "def notch_filter(frequency, fs, quality_factor=30):\n",
    "    b, a = signal.iirnotch(frequency, quality_factor, fs)\n",
    "    return b, a\n",
    "\n",
    "def apply_notch_filter(data, frequency, fs, quality_factor=30):\n",
    "    b, a = notch_filter(frequency, fs, quality_factor)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def scipy_notch_filter(data, fs, frequency, quality):\n",
    "    return scipy.signal.filtfilt(*scipy.signal.iirnotch(frequency / (fs / 2), quality), data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency response and plot of random patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Ensure reproducibility\n",
    "\n",
    "random_patient = random.choice(list(ecg_data.keys()))\n",
    "patient_signal = ecg_data[random_patient][\"ecg_signals\"]\n",
    "\n",
    "temp_freq_plot(signal = patient_signal, title=random_patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filters\n",
    "for patient in ecg_data.keys():\n",
    "\n",
    "    filtered_signal = ecg_data[patient]['ecg_signals']#.apply(lambda x: signal.detrend(x))\n",
    "    filtered_signal= filtered_signal.apply(lambda x:  signal.detrend(x))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: highpass_filter(data = x, cutoff=5, fs=fs))\n",
    "\n",
    "    filtered_signal = filtered_signal.apply(lambda x: lowpass_filter(data = x, cutoff=140, fs=fs))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: scipy_notch_filter(data=x, frequency=50, fs=fs, quality=30))\n",
    "    ecg_data[patient]['ecg_signals_filtered'] = filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_signal_filtered = ecg_data[random_patient]['ecg_signals_filtered']\n",
    "\n",
    "temp_freq_plot(signal = patient_signal_filtered, title = random_patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = [patient_data['diagnosis'] for patient_data in ecg_data.values()]\n",
    "\n",
    "# Count unique diagnoses\n",
    "from collections import Counter\n",
    "diagnosis_counts = pd.Series(diagnoses).value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Print results\n",
    "for diagnosis, count in diagnosis_counts.items():\n",
    "    print(f\"Diagnosis code {diagnosis}: {count} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_counts.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/marcpalomercadenas/Desktop/ML/utils')\n",
    "%matplotlib inline\n",
    "import auto_EDA as eda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, title=None, xlabel=None, color='#4A90E2', figsize=(10,6)):\n",
    "    \"\"\"\n",
    "    Simple histogram with optional mean line.\n",
    "    \n",
    "    Parameters:\n",
    "    data: array-like - Your data\n",
    "    title: str - Plot title\n",
    "    xlabel: str - X-axis label\n",
    "    color: str - Bar color\n",
    "    figsize: tuple - Figure size (width, height)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.hist(data, bins='auto', color=color, alpha=0.7, edgecolor='white')\n",
    "    \n",
    "    # Add mean line\n",
    "    mean = np.mean(data)\n",
    "    plt.axvline(mean, color='red', linestyle='--', label=f'Mean: {mean:.2f}')\n",
    "    \n",
    "    # Labels\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if xlabel:\n",
    "        plt.xlabel(xlabel)\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis = [patient_data['diagnosis'] for patient_data in ecg_data.values()]\n",
    "\n",
    "counts = pd.Series(diagnosis).value_counts()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "counts.plot(kind='bar')\n",
    "plt.title('Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [int(patient_data['metadata']['age']) for patient_data in ecg_data.values()]\n",
    "# Use it like:\n",
    "plot_histogram(ages, title='Age Distribution', xlabel='Age')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Extracted:\n",
    "\n",
    "Average Heart Rate (bpm): The average heart rate over the entire ECG recording.\n",
    "RR Intervals (s): The time intervals between successive R-peaks.\n",
    "SDNN (s): The standard deviation of the RR intervals, a common HRV metric.\n",
    "RMSSD (s): The root mean square of successive differences of RR intervals, a measure of HRV.\n",
    "LF Power: Power in the low-frequency band (0.04–0.15 Hz), associated with sympathetic modulation.\n",
    "HF Power: Power in the high-frequency band (0.15–0.4 Hz), associated with parasympathetic (vagal) activity.\n",
    "LF/HF Ratio: The ratio of low-frequency to high-frequency power, often used as a measure of autonomic balance.\n",
    "Average ST-Segment Deviation: The average deviation of the ST segment after each R-peak, which can be indicative of ischemic conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks, welch\n",
    "\n",
    "# Helper functions from before\n",
    "def detect_r_peaks_wfdb(ecg_signal, sampling_rate):\n",
    "    r_peaks, _ = find_peaks(ecg_signal, distance=sampling_rate * 0.6)\n",
    "    return r_peaks\n",
    "\n",
    "def compute_rr_intervals_wfdb(r_peaks, sampling_rate):\n",
    "    rr_intervals = np.diff(r_peaks) / sampling_rate\n",
    "    return rr_intervals\n",
    "\n",
    "def compute_heart_rate_wfdb(rr_intervals):\n",
    "    hr = 60 / rr_intervals\n",
    "    return hr\n",
    "\n",
    "def compute_hrv_wfdb(rr_intervals):\n",
    "    sdnn = np.std(rr_intervals)\n",
    "    rmssd = np.sqrt(np.mean(np.diff(rr_intervals) ** 2))\n",
    "    return sdnn, rmssd\n",
    "\n",
    "def compute_psd_wfdb(ecg_signal, sampling_rate, nperseg=256):\n",
    "    freqs, psd = welch(ecg_signal, fs=sampling_rate, nperseg=nperseg)\n",
    "    return freqs, psd\n",
    "\n",
    "def extract_frequency_band_power_wfdb(freqs, psd, band):\n",
    "    band_power = np.trapz(psd[(freqs >= band[0]) & (freqs <= band[1])], freqs[(freqs >= band[0]) & (freqs <= band[1])])\n",
    "    return band_power\n",
    "\n",
    "def compute_st_segment_deviation_wfdb(ecg_signal, r_peaks, sampling_rate):\n",
    "    st_deviation = []\n",
    "    for r_peak in r_peaks:\n",
    "        st_start = int(r_peak + 0.08 * sampling_rate)\n",
    "        st_end = int(st_start + 0.06 * sampling_rate)\n",
    "        if st_end < len(ecg_signal):\n",
    "            st_segment = ecg_signal[st_start:st_end]\n",
    "            st_deviation.append(np.mean(st_segment))\n",
    "    return st_deviation\n",
    "\n",
    "# Main function to apply feature extraction on the dataset\n",
    "def extract_features_from_ecg(data, sampling_rate):\n",
    "    results = {}\n",
    "\n",
    "    for patient_id, patient_data in data.items():\n",
    "        ecg_signals = patient_data['ecg_signals']\n",
    "        ecg_filtered_signals = patient_data['ecg_signals_filtered']\n",
    "        disease = patient_data['diagnosis']\n",
    "\n",
    "        # Extract features using Lead II for time-domain and frequency-domain features\n",
    "        lead_ii = ecg_filtered_signals['II'].values\n",
    "        \n",
    "        # Detect R-peaks and compute RR intervals (Lead II)\n",
    "        r_peaks = detect_r_peaks_wfdb(lead_ii, sampling_rate)\n",
    "        rr_intervals = compute_rr_intervals_wfdb(r_peaks, sampling_rate)\n",
    "\n",
    "        # Calculate Heart Rate and HRV (Lead II)\n",
    "        heart_rate = compute_heart_rate_wfdb(rr_intervals)\n",
    "        avg_heart_rate = np.mean(heart_rate)\n",
    "        sdnn, rmssd = compute_hrv_wfdb(rr_intervals)\n",
    "\n",
    "        # Frequency domain features (Lead II)\n",
    "        freqs, psd = compute_psd_wfdb(lead_ii, sampling_rate)\n",
    "        lf_power = extract_frequency_band_power_wfdb(freqs, psd, band=(0.04, 0.15))\n",
    "        hf_power = extract_frequency_band_power_wfdb(freqs, psd, band=(0.15, 0.4))\n",
    "        lf_hf_ratio = lf_power / hf_power if hf_power != 0 else np.inf\n",
    "\n",
    "        # ST-segment deviation for all leads\n",
    "        st_deviations = {}\n",
    "        for lead in ecg_filtered_signals.columns:\n",
    "            st_deviations[lead] = compute_st_segment_deviation_wfdb(ecg_filtered_signals[lead].values, r_peaks, sampling_rate)\n",
    "\n",
    "        avg_st_deviations = {f\"Avg_ST_D_{lead}\": np.mean(deviation) for lead, deviation in st_deviations.items() if len(deviation) > 0}\n",
    "\n",
    "        # Collect results\n",
    "        results[patient_id] = {\n",
    "            \"Average Heart Rate (bpm)\": avg_heart_rate,\n",
    "            # \"RR Intervals (s)\": rr_intervals,\n",
    "            \"SDNN (s)\": sdnn,\n",
    "            \"RMSSD (s)\": rmssd,\n",
    "            # \"LF Power\": lf_power,\n",
    "            # \"HF Power\": hf_power,\n",
    "            # \"LF/HF Ratio\": lf_hf_ratio,\n",
    "            \"Diagnosis\": disease\n",
    "        }\n",
    "\n",
    "        results[patient_id].update(avg_st_deviations)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Assume a sampling rate of 500Hz (adjust according to actual data)\n",
    "results = extract_features_from_ecg(ecg_data, fs)\n",
    "features_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(features_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = pd.read_csv(\"/Users/marcpalomercadenas/Desktop/ML/ECG_prediction/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/ConditionNames_SNOMED-CT.csv\")\n",
    "diseases=diseases.rename(columns={\"Snomed_CT\": \"Disease\"})\n",
    "diseases['Disease']=diseases['Disease'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerías:\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = features_df['Diagnosis']#.apply(lambda x: x.split(','))\n",
    "X = features_df.loc[:,features_df.columns != 'Diagnosis'] \n",
    "\n",
    "# mlb = MultiLabelBinarizer()\n",
    "# binary_labels = mlb.fit_transform(y)\n",
    "\n",
    "# label_df = pd.DataFrame(binary_labels, columns=mlb.classes_)\n",
    "# label_df\n",
    "\n",
    "print(y.value_counts())\n",
    "print(diseases[diseases['Disease'].isin([\"426783006\", \"426177001\", \"427084000\", \"164889003\", \"164934002\", \"164890007\"])])\n",
    "\n",
    "y=y[y.isin([\"426783006\", \"426177001\"])]\n",
    "X = X.loc[y.index]\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "label_to_color = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "colors = [label_to_color[label] for label in y]\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=colors)\n",
    "plt.title(\"PCA of Example Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "# plt.colorbar(scatter, label='Class Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets remake the feature extration!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividimos los conjuntos de entrenamiento(70%) y test(30). \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, label_df, test_size=0.3, random_state=42)\n",
    "# mlb.classes_\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a multi-label RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, Y_pred)\n",
    "f1 = f1_score(y_test, Y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Modelos de clasificación\n",
    "models = dict([(\"Random Forest\", RandomForestClassifier()), \n",
    "          ('Logistic Regression', LogisticRegression(max_iter=10000)), \n",
    "          ('Support Vector Machine', SVC()),\n",
    "          ('XGBoost', XGBClassifier()),\n",
    "          ('LightGBM', LGBMClassifier())])\n",
    "\n",
    "# Evaluar los modelos utilizando validación cruzada\n",
    "for name, model in models.items():\n",
    "    print(f'{name}:')\n",
    "    # Volidación cruzada\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=7, scoring = 'balanced_accuracy')\n",
    "    print(f'Mean cross-validation score: {scores.mean():.3f}')\n",
    "    print(f'Standard deviation: {scores.std():.3f}')\n",
    "  \n",
    "    #Ajuste con train:\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #Predicción con test:\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    #Resultados\n",
    "    accuracy = accuracy_score(y_test, y_pred)#Exactitud\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')#Precisión\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f'Accuracy score on test data: {accuracy:.3f}')\n",
    "    print(f'Precision score on test data: {precision:.3f}')\n",
    "    print(f'Recall score on test data: {recall:.3f}\\n')\n",
    "\n",
    "#REPETIMOS CON LOS DATOS NORMALIZADOS:\n",
    "print(' ')\n",
    "print('With scaled data')\n",
    "print(' ')\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "normalizer = Normalizer().fit(X)\n",
    "X_train_scaled=normalizer.transform(X_train_scaled)\n",
    "X_test_scaled=normalizer.transform(X_test_scaled)\n",
    "\n",
    "#Utilizamos los mismos modelos:\n",
    "models = dict([(\"Random Forest\", RandomForestClassifier()), \n",
    "          ('Logistic Regression', LogisticRegression(max_iter=10000)), \n",
    "          ('Support Vector Machine', SVC()),\n",
    "          ('XGBoost', XGBClassifier()),\n",
    "          ('LightGBM', LGBMClassifier())])\n",
    "\n",
    "# Evaluar los modelos utilizando validación cruzada\n",
    "for name, model in models.items():\n",
    "    print(f'{name}:')\n",
    "    \n",
    "    # Validación cruzada\n",
    "    scores = cross_val_score(model, X_train_scaled, y_train, cv=7, scoring='balanced_accuracy')\n",
    "    print(f'Mean cross-validation score: {scores.mean():.3f}')\n",
    "    print(f'Standard deviation: {scores.std():.3f}')\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train) #Ajuste del modelo a los datos de entrenamiento escalados y las etiquetas. \n",
    "    \n",
    "    #Predicción con los datos test escalados:\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    #Resultados:\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(f'Accuracy score on test data: {accuracy:.3f}')\n",
    "    print(f'Precision score on test data: {precision:.3f}')\n",
    "    print(f'Recall score on test data: {recall:.3f}\\n')\n",
    "\n",
    "#print(X)a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame creation\n",
    "data = {\n",
    "    'Var1': [10, 12, 7, 14, 11, 9, 15, 13, 8, 17, 12, 10, 19, 14, 11, 16, 13, 12],\n",
    "    'Var2': [5, 6, 4, 3, 8, 2, 7, 5, 6, 4, 9, 5, 8, 6, 7, 3, 10, 6],\n",
    "    'Var3': [8, 9, 6, 7, 10, 5, 12, 11, 7, 9, 14, 6, 13, 10, 8, 11, 12, 9],\n",
    "    'Var4': [1, 2, 3, 4, 5, 6, 8, 7, 2, 3, 6, 4, 5, 8, 7, 4, 5, 8],\n",
    "    'Var5': [7, 5, 9, 6, 8, 10, 11, 9, 12, 10, 8, 7, 14, 9, 12, 11, 10, 8],\n",
    "    'Cat1': ['a', 'b', 'c', 'd', 'e', 'f', 'a', 'b', 'c', 'd', 'e', 'f', 'a', 'b', 'c', 'd', 'e', 'f']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Intra-group Variability\n",
    "intra_group_stats = df.groupby('Cat1').agg({\n",
    "    'Var1': ['mean', 'std'],\n",
    "    'Var2': ['mean', 'std'],\n",
    "    'Var3': ['mean', 'std'],\n",
    "    'Var4': ['mean', 'std'],\n",
    "    'Var5': ['mean', 'std']\n",
    "})\n",
    "\n",
    "# Calculate CV (Coefficient of Variation)\n",
    "for var in ['Var1', 'Var2', 'Var3', 'Var4', 'Var5']:\n",
    "    intra_group_stats[(var, 'CV')] = (intra_group_stats[(var, 'std')] / intra_group_stats[(var, 'mean')]) * 100\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "intra_group_stats.columns = ['_'.join(col).strip() for col in intra_group_stats.columns.values]\n",
    "\n",
    "# Convert intra_group_stats to DataFrame\n",
    "intra_group_df = intra_group_stats.reset_index()\n",
    "print(\"Intra-group Variability:\\n\", intra_group_df)\n",
    "\n",
    "# 2. Inter-group Variability\n",
    "# Calculate the mean of means for each variable\n",
    "group_means = df.groupby('Cat1').mean()\n",
    "\n",
    "# Compute variance of group means across Cat1 groups\n",
    "variance_between_groups = group_means.var()\n",
    "\n",
    "# Calculate overall means for each variable\n",
    "overall_means = df[['Var1', 'Var2', 'Var3', 'Var4', 'Var5']].mean()\n",
    "\n",
    "# Calculate CV for the variance of group means\n",
    "cv_between_groups = (variance_between_groups / overall_means) * 100\n",
    "\n",
    "# Create a DataFrame for inter-group variability\n",
    "inter_group_df = pd.DataFrame({\n",
    "    'Variance_Between_Groups': variance_between_groups,\n",
    "    'CV_Between_Groups': cv_between_groups\n",
    "}).reset_index()\n",
    "\n",
    "print(\"\\nInter-group Variability:\\n\", inter_group_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
