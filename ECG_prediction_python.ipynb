{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TO DO :\n",
    "\n",
    "\n",
    "\n",
    "     - Separate data into metadata df and signal dict, so that it is easier to analyse data and to access signal just with key (sample_id) DONE\n",
    "\n",
    "     -  Check for normal ecgs and train the classifier with AF vs normal (NORMAL ECGS ARE STATED AS SINUS RYTHM)\n",
    "\n",
    "     - Only 100 samples are loaded for velocity. All should be loaded at the end DONE\n",
    "\n",
    "     - If waiting time is too long use a sql based system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -V > full_requirements.txt && pip list --format=freeze >> full_requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb  # To read the .hea file\n",
    "import scipy.io as sio  # To read .mat files\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, lfilter\n",
    "import numpy as np\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import signal \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Directories\n",
    "dataset_dir = '../a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/WFDBRecords'\n",
    "\n",
    "g_leads = ['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6']\n",
    "fs = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_map_path = '../a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/ConditionNames_SNOMED-CT.csv'\n",
    "disease_map = pd.read_csv(disease_map_path)\n",
    "disease_map = disease_map.set_index('Snomed_CT')\n",
    "def disease_map_func(diagnosis, disease_map = disease_map):\n",
    "\n",
    "\n",
    "    diagnosis = [disease_map.loc[int(code), \"Full Name\"] for code in diagnosis.split(',')]\n",
    "    return diagnosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data = {}\n",
    "patient_data = {}\n",
    "\n",
    "# Loop through each record in the dataset directory\n",
    "for directory1 in sorted([dir for dir in os.listdir(f\"{dataset_dir}\") if not dir.startswith('.')])[0:3]: #errase the [0:3] to get all the files\n",
    "    for directory2 in sorted([dir for dir in os.listdir(f\"{dataset_dir}/{directory1}\") if not dir.startswith('.')]):\n",
    "        for record_hea in [dir for dir in os.listdir(f\"{dataset_dir}/{directory1}/{directory2}\") if not dir.startswith('.')]:\n",
    "            if record_hea.endswith(\".hea\"):  # Process .hea files\n",
    "                try:\n",
    "                    patient_id = record_hea.split(\".hea\")[0]\n",
    "                    record_path = f\"{dataset_dir}/{directory1}/{directory2}/{patient_id}\"\n",
    "\n",
    "                    # Read the header (.hea) file\n",
    "                    record = wfdb.rdheader(record_path)\n",
    "\n",
    "                    # Extract metadata\n",
    "                    age = None\n",
    "                    sex = None\n",
    "                    diagnosis = None\n",
    "                    for comment in record.comments:\n",
    "                        if comment.startswith(\"Age:\"):\n",
    "                            age = comment.split(\":\")[1].strip()\n",
    "                        if comment.startswith(\"Sex:\"):\n",
    "                            sex = comment.split(\":\")[1].strip()\n",
    "                        if comment.startswith(\"Dx:\"):\n",
    "                            diagnosis = comment.split(\":\")[1].strip()  # This gives you the SNOMED codes\n",
    "                    \n",
    "                    # Read the 12-lead ECG signal from the .mat file\n",
    "                    mat_file_path = f\"{record_path}.mat\"\n",
    "                    mat_data = sio.loadmat(mat_file_path)\n",
    "                    ecg_signals = mat_data['val']  # 'val' typically holds the ECG signal in PhysioNet datasets\n",
    "                    ecg_signals= pd.DataFrame(ecg_signals.T, columns  = g_leads)\n",
    "                    # Store the data in the dictionary\n",
    "                    ecg_data[patient_id] = {\n",
    "                        \"ecg_signals\": ecg_signals,  # 12-lead ECG signals\n",
    "                        }\n",
    "                    patient_data[patient_id] = {\n",
    "                        \"diagnosis_code\": [disease for disease in diagnosis.split(',')],  # Disease label (SNOMED codes)\n",
    "                        \"diagnosis_name\": disease_map_func(diagnosis),\n",
    "                        \"age\": age,\n",
    "                        \"sex\": sex\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"{patient_id}: {e}\")\n",
    "\n",
    "patient_data=pd.DataFrame(patient_data).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(patient_data).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import scipy\n",
    "import random\n",
    "from scipy.signal import find_peaks, welch\n",
    "\n",
    "def temp_freq_plot(signal, title):\n",
    "\n",
    "    fig,ax = plt.subplots(12,2, figsize=(20,12*4))\n",
    "    for i in range(0,12):\n",
    "        _key = g_leads[i]\n",
    "        #ax[i].set_title(_key)\n",
    "        ax[i,0].plot(signal[_key], color='black', linewidth=0.6)\n",
    "        ax[i,0].set_ylim((-1500,1500))\n",
    "        ax[i,0].set_xticks(   np.arange(0,5001,500)  )   \n",
    "        ax[i,0].set_xticklabels(   np.arange(0,5001,500)/fs  )   \n",
    "        ax[i,0].grid(axis='x')\n",
    "        ax[i,0].annotate(_key,(-200,0))\n",
    "        #ax[i].set_xlabel('Time(sec)')\n",
    "        #ax[i].set_ylabel('mV')\n",
    "        ax[i,0].hlines(0,0,5000,color='black', linewidth=0.3)\n",
    "\n",
    "        frequencies, psd_values = welch( signal[_key], fs, nperseg=1024)\n",
    "\n",
    "        # Plotting the estimated PSD\n",
    "        ax[i,1].semilogy(frequencies, psd_values)\n",
    "        ax[i,1].set_title('Power Spectral Density (PSD) Estimate using Welch\\'s Method')\n",
    "        ax[i,1].set_xlabel('Frequency (Hz)')\n",
    "        ax[i,1].set_ylabel('PSD (V^2/Hz)')\n",
    "\n",
    "    plt.suptitle(f\"{title}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyquist  # Normalize cutoff frequency\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='lowpass', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "# Design a Notch Filter to remove 60 Hz power line interference\n",
    "def notch_filter(frequency, fs, quality_factor=30):\n",
    "    b, a = signal.iirnotch(frequency, quality_factor, fs)\n",
    "    return b, a\n",
    "\n",
    "def apply_notch_filter(data, frequency, fs, quality_factor=30):\n",
    "    b, a = notch_filter(frequency, fs, quality_factor)\n",
    "    y = signal.filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "def scipy_notch_filter(data, fs, frequency, quality):\n",
    "    return scipy.signal.filtfilt(*scipy.signal.iirnotch(frequency / (fs / 2), quality), data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Signal filtering pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Frequency response and plot of random patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Ensure reproducibility\n",
    "\n",
    "random_patient = random.choice(list(ecg_data.keys()))\n",
    "patient_signal = ecg_data[random_patient][\"ecg_signals\"]\n",
    "\n",
    "temp_freq_plot(signal = patient_signal, title=random_patient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filters\n",
    "for patient in ecg_data.keys():\n",
    "\n",
    "    filtered_signal = ecg_data[patient]['ecg_signals']#.apply(lambda x: signal.detrend(x))\n",
    "    filtered_signal= filtered_signal.apply(lambda x:  signal.detrend(x))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: highpass_filter(data = x, cutoff=5, fs=fs))\n",
    "\n",
    "    filtered_signal = filtered_signal.apply(lambda x: lowpass_filter(data = x, cutoff=140, fs=fs))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: scipy_notch_filter(data=x, frequency=50, fs=fs, quality=30))\n",
    "    ecg_data[patient]['ecg_signals_filtered'] = filtered_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_signal_filtered = ecg_data[random_patient]['ecg_signals_filtered']\n",
    "\n",
    "temp_freq_plot(signal = patient_signal_filtered, title = random_patient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Illness distribution and data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/marcpalomer/Documents/Personal/ECG_prediction/utils')\n",
    "import auto_EDA as eda \n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_arrhythmia(diagnosis_codes):\n",
    "   # Arrhythmia SNOMED codes\n",
    "    #    snomed_codes = {\n",
    "    #    'AF': '49436004',\n",
    "    #    'RBBB': '59118001', \n",
    "    #    'LBBB': '28189009',\n",
    "    #    'IAVB': '270492004',\n",
    "    #    'PAC': '284470004',\n",
    "    #    'PVC': '427172004',\n",
    "    #    'MI': '22298006'\n",
    "    #}\n",
    "   arrhythmia_codes = ['49436004', '59118001', '28189009', '270492004', '284470004', '427172004', '22298006']\n",
    "   \n",
    "   # Sinus Rhythm Normal code\n",
    "   healthy_code = '426783006'\n",
    "   \n",
    "   # Check if any arrhythmia code is present\n",
    "   if any(code in diagnosis_codes for code in arrhythmia_codes):\n",
    "       return 'Arrhythmia'\n",
    "   # Check if healthy code present\n",
    "   elif healthy_code in diagnosis_codes:\n",
    "       return 'Healthy'\n",
    "   # Otherwise other\n",
    "   else:\n",
    "       return 'Other'\n",
    "\n",
    "# Apply to dataframe\n",
    "patient_data['arrhythmia'] = patient_data['diagnosis_code'].apply(classify_arrhythmia)\n",
    "patient_data['arrhythmia'].hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### HRV calculation in lead II (gold standard) (it is a time saries for patient so like a new lead...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "\n",
    "def validate_rpeaks(rpeaks, fs):\n",
    "    # Remove physiologically impossible R-peaks\n",
    "    rr_intervals = np.diff(rpeaks) / fs\n",
    "    valid_rr = (rr_intervals >= 0.2) & (rr_intervals <= 2.0)  \n",
    "    valid_peaks = rpeaks[1:][valid_rr]\n",
    "    return valid_peaks\n",
    "\n",
    "def calculate_hr_metrics(rpeaks, fs):\n",
    "    rr_intervals = np.diff(rpeaks) / fs\n",
    "    hr = 60 / rr_intervals\n",
    "    return np.median(hr), np.mean(hr), np.std(hr), np.min(hr), np.max(hr)\n",
    "\n",
    "def calculate_heartrate(record, fs):\n",
    "    # Find R-peaks using neurokit2\n",
    "    rpeaks = list(nk.ecg_findpeaks(record, sampling_rate=fs).values())[0]\n",
    "    rpeaks = validate_rpeaks(rpeaks, fs)\n",
    "    return calculate_hr_metrics(rpeaks, fs)\n",
    "\n",
    "def add_hr_metrics(patient_data, ecg_data):\n",
    "    metrics = {'median_hr': [], 'mean_hr': [], 'std_hr': [], 'min_hr': [], 'max_hr': []}\n",
    "    \n",
    "    for id in patient_data.index:\n",
    "        if id in ecg_data:\n",
    "            lead_II = ecg_data[id]['ecg_signals_filtered'].loc[:,'II']\n",
    "            try:\n",
    "                median_hr, mean_hr, std_hr, min_hr, max_hr = calculate_heartrate(lead_II, fs=500)\n",
    "            except:\n",
    "                median_hr, mean_hr, std_hr, min_hr, max_hr = [np.nan,np.nan,np.nan,np.nan,np.nan]\n",
    "            metrics['median_hr'].append(median_hr)\n",
    "            metrics['mean_hr'].append(mean_hr)\n",
    "            metrics['std_hr'].append(std_hr)\n",
    "            metrics['min_hr'].append(min_hr)\n",
    "            metrics['max_hr'].append(max_hr)\n",
    "        else:\n",
    "            for key in metrics:\n",
    "                metrics[key].append(None)\n",
    "    \n",
    "    for metric, values in metrics.items():\n",
    "        patient_data[metric] = values\n",
    "    \n",
    "    return patient_data\n",
    "\n",
    "patient_data = add_hr_metrics(patient_data, ecg_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data['Healthy'] = ['HEALTHY' if 'Health' in col else 'ILL' for col in patient_data['arrhythmia'] ]\n",
    "patient_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import automatic_reporting as AR \n",
    "# reportName = \"Patient Data and HRV\"\n",
    "analyse_features= ['median_hr',\t'mean_hr','std_hr',\t'min_hr', 'max_hr', 'age']\n",
    "# control_features= ['sex', 'arrhythmia', 'Healthy']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PATH = '.'\n",
    "\n",
    "# report_builder = AR.AutoReport(main_path = f\"{PATH}\",\n",
    "#                                     data = patient_data,\n",
    "#                                     analyse_features= analyse_features,\n",
    "#                                     control_features= control_features)\n",
    "\n",
    "# report_builder.generate_report(ReportName = reportName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dumb classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_dataset = patient_data[analyse_features+['Healthy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "def quick_classify(df, target_col, features=None):\n",
    "    \"\"\"\n",
    "    Quick classification using RandomForest with minimal preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame with your data\n",
    "        target_col: name of the column to predict\n",
    "        features: list of feature columns to use (optional, uses all except target if None)\n",
    "    \"\"\"\n",
    "    # Select features\n",
    "    if features is None:\n",
    "        features = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle non-numeric columns\n",
    "    # X = pd.get_dummies(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': X.columns.tolist()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_classify_dict = quick_classify(ML_dataset, target_col='Healthy')\n",
    "quick_classify_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import RF_pipeline\n",
    "\n",
    "def analyze_model(df, target_col, features=None, test_size=0.2, random_state=42):\n",
    "    \"\"\"Main function to analyze the model.\"\"\"\n",
    "    if features is None:\n",
    "        features = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Preprocess data\n",
    "    X_processed = X.copy()\n",
    "    for column in X_processed.columns:\n",
    "        X_processed[column] = pd.to_numeric(X_processed[column], errors='coerce')\n",
    "    X_processed = X_processed.fillna(X_processed.median())\n",
    "    y_processed = pd.factorize(y)[0]\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    analyzer = RF_pipeline.RandomForestAnalyzer()\n",
    "    visualizer = RF_pipeline.ModelVisualizer()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, analyzer, visualizer, features\n",
    "   \n",
    "X_train, X_test, y_train, y_test, analyzer, visualizer, features = analyze_model(df=ML_dataset, target_col='Healthy', features=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB Analysis\n",
    "print(\"Performing OOB Analysis...\")\n",
    "oob_results = analyzer.analyze_oob(X_train, y_train)\n",
    "visualizer.plot_oob_analysis(oob_results)\n",
    "\n",
    "# Feature Importance Analysis\n",
    "print(\"\\nAnalyzing Feature Importance...\")\n",
    "importance_results = analyzer.analyze_feature_importance(X_train, y_train)\n",
    "visualizer.plot_feature_importance(importance_results)\n",
    "\n",
    "# Cross Validation\n",
    "print(\"\\nPerforming Cross Validation...\")\n",
    "cv_results = analyzer.cross_validate(X_train, y_train)\n",
    "visualizer.plot_cv_results(cv_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As we have seen, the distribution in cv of the performance metrics is better than the performance over the uncrossvalidated healthy, also pointing to the fact that the model is better at predicting the majority class than the minority. Anyway it is incredibly good at both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This goes inline with the PCA..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Decision boundary display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Boundary Analysis\n",
    "print(\"\\nVisualizing Decision Boundaries...\")\n",
    "model = analyzer.get_fitted_model(X_train, y_train)\n",
    "\n",
    "print(\"PCA-based boundaries:\")\n",
    "boundary_viz = RF_pipeline.BoundaryVisualizer()\n",
    "X_pca = boundary_viz.plot_boundaries_2d(X_train, y_train, model, method='pca')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Shap values:\n",
    "\n",
    " - What favours arrythmia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "X_sub = shap.sample(X_train)\n",
    "explainer = shap.Explainer(model.predict_proba, X_sub)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "class_index = 1\n",
    "data_index = 1\n",
    "\n",
    "shap.plots.waterfall(shap_values[data_index,:,class_index], )\n",
    "\n",
    "shap.initjs()\n",
    "shap.plots.force(shap_values[data_index,:,class_index])\n",
    "\n",
    "shap.plots.beeswarm(shap_values[:,:,class_index])\n",
    "\n",
    "shap.plots.bar(shap_values[:,:,class_index])\n",
    "\n",
    "shap.plots.scatter(shap_values[:, 'max_hr',1])\n",
    "\n",
    "shap.plots.scatter(shap_values[:, 'max_hr',1], color=shap_values[:,:,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DL CNN prediction:\n",
    "\n",
    " - Will we beat HRV RF classification? (With only 1 lead!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ## Pipeline Structure\n",
    "\n",
    "\n",
    "\n",
    " ### Training Phase\n",
    "\n",
    " 1. **Data Preparation**\n",
    "\n",
    "   - Raw ECG dictionary + labels → `prepare_data()` → `normalize_signals()`\n",
    "\n",
    "   - Train/val/test split\n",
    "\n",
    "   - Dataset & DataLoader creation for batching\n",
    "\n",
    "\n",
    "\n",
    " 2. **Training Cycle**\n",
    "\n",
    "   - DataLoader feeds batches to ModelTrainer\n",
    "\n",
    "   - Forward pass through ECGNet\n",
    "\n",
    "   - Loss calculation, backpropagation\n",
    "\n",
    "   - Validation performance check\n",
    "\n",
    "   - Save best model\n",
    "\n",
    "   - Track metrics history\n",
    "\n",
    "\n",
    "\n",
    " ### Evaluation Phase\n",
    "\n",
    " 1. **Model Assessment**\n",
    "\n",
    "   - Load best model weights\n",
    "\n",
    "   - Full forward pass on test set\n",
    "\n",
    "   - Generate predictions/probabilities\n",
    "\n",
    "\n",
    "\n",
    " 2. **Results**\n",
    "\n",
    "   - Performance metrics calculation\n",
    "\n",
    "   - Visualization generation\n",
    "\n",
    "   - Save all results\n",
    "\n",
    "\n",
    "\n",
    " ## DL Architecture\n",
    "\n",
    "\n",
    "\n",
    " ### Input Processing\n",
    "\n",
    " - 12-lead ECG signals\n",
    "\n",
    " - 5000 timepoints per lead\n",
    "\n",
    " - Normalized per lead\n",
    "\n",
    "\n",
    "\n",
    " ### Feature Extraction\n",
    "\n",
    " - Conv1d (k=50): QRS complex detection\n",
    "\n",
    " - Conv1d (k=7): Wave morphology\n",
    "\n",
    " - Conv1d (k=5): Fine details\n",
    "\n",
    " - Increasing channels (12→32→64→128) for feature hierarchy\n",
    "\n",
    "\n",
    "\n",
    " ### Each Conv Block\n",
    "\n",
    " - BatchNorm: Training stability\n",
    "\n",
    " - ReLU: Non-linearity\n",
    "\n",
    " - MaxPool: Dimension reduction\n",
    "\n",
    "\n",
    "\n",
    " ### Classification\n",
    "\n",
    " - AdaptivePool: Fixed output size\n",
    "\n",
    " - FC layers (6400→256→64→2)\n",
    "\n",
    " - Dropout layers prevent overfitting\n",
    "\n",
    " - Output: Binary classification probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your labels\n",
    "labels_dict = patient_data['Healthy'].reset_index(drop=False)\n",
    "labels_dict = labels_dict.rename(columns={'Healthy':'label'})\n",
    "\n",
    "# Save categories before encoding\n",
    "categories = pd.Categorical(labels_dict['label']).categories\n",
    "\n",
    "# Encode labels\n",
    "labels_dict['label'] = pd.Categorical(labels_dict['label']).codes\n",
    "\n",
    "# Create and print the encoding dictionary\n",
    "encoding_dict = dict(enumerate(categories))\n",
    "\n",
    "print(\"\\nLabel encoding dictionary:\")\n",
    "for code, label in encoding_dict.items():\n",
    "    print(f\"{label} -> {code}\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(labels_dict['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### DL model loading (or training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data['JS00067'].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import CNN\n",
    "import traceback\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "importlib.reload(CNN)\n",
    "\n",
    "\n",
    "\n",
    "# Verify model path\n",
    "model_path = \"/Users/marcpalomer/Documents/Personal/ECG_prediction/Results/DL_model/results_20241214_202359/best_model.pth\"\n",
    "print(f\"Model file exists: {Path(model_path).exists()}\")\n",
    "\n",
    "# Configuration settings\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    print(f\"Loading pretrained model from {model_path}\")\n",
    "    results = CNN.main(ecg_data, labels_dict, config, load_model_path=model_path)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults keys:\", results.keys() if results else \"No results\")\n",
    "    model = results['model']\n",
    "    output_dir = results['output_dir']\n",
    "    importance_results = results['importance_results']\n",
    "\n",
    "    print(f\"\\nResults saved in: {output_dir}\")\n",
    "    if 'test_metrics' in results:\n",
    "        print(\"\\nTest Metrics:\")\n",
    "        print(f\"ROC AUC: {results['test_metrics']['roc_auc']:.4f}\")\n",
    "        print(f\"PR AUC: {results['test_metrics']['pr_auc']:.4f}\")\n",
    "        print(f\"Test Loss: {results['test_metrics']['test_loss']:.4f}\")\n",
    "\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### External validation Performance\n",
    "\n",
    "\n",
    "\n",
    " - Try performance on other 4 random directories of samples to assess actual performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([dir for dir in os.listdir(f\"{dataset_dir}\") if not dir.startswith('.')])[-3::]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_validation_ecg_data = {}\n",
    "external_validation_data = {}\n",
    "\n",
    "# Loop through each record in the dataset directory\n",
    "for directory1 in sorted([dir for dir in os.listdir(f\"{dataset_dir}\") if not dir.startswith('.')])[-10::-6]: #errase the [0:3] to get all the files\n",
    "    for directory2 in sorted([dir for dir in os.listdir(f\"{dataset_dir}/{directory1}\") if not dir.startswith('.')]):\n",
    "        for record_hea in [dir for dir in os.listdir(f\"{dataset_dir}/{directory1}/{directory2}\") if not dir.startswith('.')]:\n",
    "            if record_hea.endswith(\".hea\"):  # Process .hea files\n",
    "                try:\n",
    "                    patient_id = record_hea.split(\".hea\")[0]\n",
    "                    record_path = f\"{dataset_dir}/{directory1}/{directory2}/{patient_id}\"\n",
    "\n",
    "                    # Read the header (.hea) file\n",
    "                    record = wfdb.rdheader(record_path)\n",
    "\n",
    "                    # Extract metadata\n",
    "                    age = None\n",
    "                    sex = None\n",
    "                    diagnosis = None\n",
    "                    for comment in record.comments:\n",
    "                        if comment.startswith(\"Age:\"):\n",
    "                            age = comment.split(\":\")[1].strip()\n",
    "                        if comment.startswith(\"Sex:\"):\n",
    "                            sex = comment.split(\":\")[1].strip()\n",
    "                        if comment.startswith(\"Dx:\"):\n",
    "                            diagnosis = comment.split(\":\")[1].strip()  # This gives you the SNOMED codes\n",
    "                    \n",
    "                    # Read the 12-lead ECG signal from the .mat file\n",
    "                    mat_file_path = f\"{record_path}.mat\"\n",
    "                    mat_data = sio.loadmat(mat_file_path)\n",
    "                    ecg_signals = mat_data['val']  # 'val' typically holds the ECG signal in PhysioNet datasets\n",
    "                    ecg_signals= pd.DataFrame(ecg_signals.T, columns  = g_leads)\n",
    "                    # Store the data in the dictionary\n",
    "                    external_validation_ecg_data[patient_id] = {\n",
    "                        \"ecg_signals\": ecg_signals,  # 12-lead ECG signals\n",
    "                        }\n",
    "                    external_validation_data[patient_id] = {\n",
    "                        \"diagnosis_code\": [disease for disease in diagnosis.split(',')],  # Disease label (SNOMED codes)\n",
    "                        \"diagnosis_name\": disease_map_func(diagnosis),\n",
    "                        \"age\": age,\n",
    "                        \"sex\": sex\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"{patient_id}: {e}\")\n",
    "\n",
    "external_validation_data=pd.DataFrame(external_validation_data).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filters\n",
    "for patient in external_validation_ecg_data.keys():\n",
    "\n",
    "    filtered_signal = external_validation_ecg_data[patient]['ecg_signals']#.apply(lambda x: signal.detrend(x))\n",
    "    filtered_signal= filtered_signal.apply(lambda x:  signal.detrend(x))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: highpass_filter(data = x, cutoff=5, fs=fs))\n",
    "\n",
    "    filtered_signal = filtered_signal.apply(lambda x: lowpass_filter(data = x, cutoff=140, fs=fs))\n",
    "    filtered_signal = filtered_signal.apply(lambda x: scipy_notch_filter(data=x, frequency=50, fs=fs, quality=30))\n",
    "    external_validation_ecg_data[patient]['ecg_signals_filtered'] = filtered_signal\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_arrhythmia(diagnosis_codes):\n",
    "   # Arrhythmia SNOMED codes\n",
    "    #    snomed_codes = {\n",
    "    #    'AF': '49436004',\n",
    "    #    'RBBB': '59118001', \n",
    "    #    'LBBB': '28189009',\n",
    "    #    'IAVB': '270492004',\n",
    "    #    'PAC': '284470004',\n",
    "    #    'PVC': '427172004',\n",
    "    #    'MI': '22298006'\n",
    "    #}\n",
    "   arrhythmia_codes = ['49436004', '59118001', '28189009', '270492004', '284470004', '427172004', '22298006']\n",
    "   \n",
    "   # Sinus Rhythm Normal code\n",
    "   healthy_code = '426783006'\n",
    "   \n",
    "   # Check if any arrhythmia code is present\n",
    "   if any(code in diagnosis_codes for code in arrhythmia_codes):\n",
    "       return 'Arrhythmia'\n",
    "   # Check if healthy code present\n",
    "   elif healthy_code in diagnosis_codes:\n",
    "       return 'Healthy'\n",
    "   # Otherwise other\n",
    "   else:\n",
    "       return 'Other'\n",
    "\n",
    "# Apply to dataframe\n",
    "external_validation_data['arrhythmia'] = external_validation_data['diagnosis_code'].apply(classify_arrhythmia)\n",
    "external_validation_data['arrhythmia'].hist()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_validation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_validation_labels_dict = external_validation_data['arrhythmia'].reset_index(drop=False)\n",
    "\n",
    "external_validation_labels_dict['label'] = ['HEALTHY' if 'Health' in col else 'ILL' for col in external_validation_labels_dict['arrhythmia'] ]\n",
    "external_validation_labels_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save categories before encoding\n",
    "categories = pd.Categorical(external_validation_labels_dict['label']).categories\n",
    "\n",
    "# Encode labels\n",
    "external_validation_labels_dict['label'] = pd.Categorical(external_validation_labels_dict['label']).codes\n",
    "\n",
    "# Create and print the encoding dictionary\n",
    "encoding_dict = dict(enumerate(categories))\n",
    "\n",
    "print(\"\\nLabel encoding dictionary:\")\n",
    "for code, label in encoding_dict.items():\n",
    "    print(f\"{label} -> {code}\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(external_validation_labels_dict['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_validation_labels_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import CNN\n",
    "import traceback\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Reload the module to get the latest changes\n",
    "importlib.reload(CNN)\n",
    "\n",
    "\n",
    "\n",
    "# Verify model path\n",
    "model_path = \"/Users/marcpalomer/Documents/Personal/ECG_prediction/Results/DL_model/results_20241214_202359/best_model.pth\"\n",
    "print(f\"Model file exists: {Path(model_path).exists()}\")\n",
    "\n",
    "# Configuration settings\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.001,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Run the main function\n",
    "try:\n",
    "    print(f\"Loading pretrained model from {model_path}\")\n",
    "    results = CNN.main(external_validation_ecg_data, external_validation_labels_dict, config, load_model_path=model_path)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults keys:\", results.keys() if results else \"No results\")\n",
    "    model = results['model']\n",
    "    output_dir = results['output_dir']\n",
    "    importance_results = results['importance_results']\n",
    "\n",
    "    print(f\"\\nResults saved in: {output_dir}\")\n",
    "    if 'test_metrics' in results:\n",
    "        print(\"\\nTest Metrics:\")\n",
    "        print(f\"ROC AUC: {results['test_metrics']['roc_auc']:.4f}\")\n",
    "        print(f\"PR AUC: {results['test_metrics']['pr_auc']:.4f}\")\n",
    "        print(f\"Test Loss: {results['test_metrics']['test_loss']:.4f}\")\n",
    "\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
